{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ukrainian-to-English translation with Transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnoE8S0s3yoN",
        "outputId": "300e0e45-4b08-48da-840e-156238ae85ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from keras.models import load_model\n"
      ],
      "metadata": {
        "id": "u5NeTrmH30om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/gdrive/MyDrive/TheSocialDilemma/ukr.txt\") as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]"
      ],
      "metadata": {
        "id": "fObc3-I35J9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, ukr = line.split(\"\\t\")[:2]\n",
        "    ukr = \"[start] \" + ukr + \" [end]\"\n",
        "    text_pairs.append((eng, ukr))"
      ],
      "metadata": {
        "id": "NHgruZry9phw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dawx6q-N-pbV",
        "outputId": "d55a2593-ba26-4f4f-bfd9-f25a17ff60bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"I didn't ask Tom any questions.\", '[start] Я не поставив Тому жодних запитань. [end]')\n",
            "('That sounds fun.', '[start] Це має бути весело. [end]')\n",
            "('Tom let Mary go.', '[start] Том відпустив Мері. [end]')\n",
            "('Examine them.', '[start] Перевірте їх. [end]')\n",
            "('Stop looking at me.', '[start] Припини на мене дивитися. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQauECR8-xtf",
        "outputId": "f4a90a55-cb41-4f8a-d2c0-110eb2ceacf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "154461 total pairs\n",
            "108123 training pairs\n",
            "23169 validation pairs\n",
            "23169 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "vocab_size = 17000\n",
        "sequence_length = 25\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
        ")\n",
        "spa_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_spa_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "spa_vectorization.adapt(train_spa_texts)"
      ],
      "metadata": {
        "id": "W7ThH0Tp-yvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(eng, spa):\n",
        "    eng = eng_vectorization(eng)\n",
        "    spa = spa_vectorization(spa)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "TaC80agd-1mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf_TNnHW-3ug",
        "outputId": "52419ea5-0de9-4a8e-8832-acc098c300f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'embed_dim ': self.embed_dim,\n",
        "            'dense_dim': self.dense_dim,\n",
        "            'num_heads': self.num_heads,\n",
        "            'attention': self.attention, \n",
        "            'dense_proj': self.dense_proj, \n",
        "            'layernorm_1': self.layernorm_1, \n",
        "            'layernorm_2': self.layernorm_2, \n",
        "            'supports_masking': self.supports_masking,\n",
        "        }) \n",
        "        return config\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'token_embeddings': self.token_embeddings,\n",
        "            'position_embeddings': self.position_embeddings,\n",
        "            'sequence_length': self.sequence_length,\n",
        "            'vocab_size': self.vocab_size, \n",
        "            'embed_dim': self.embed_dim, \n",
        "        }) \n",
        "        return config\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'embed_dim ': self.embed_dim,\n",
        "            'latent_dim': self.latent_dim,\n",
        "            'num_heads': self.num_heads,\n",
        "            'attention_1': self.attention_1,\n",
        "            'attention_2': self.attention_2,  \n",
        "            'dense_proj': self.dense_proj, \n",
        "            'layernorm_1': self.layernorm_1, \n",
        "            'layernorm_2': self.layernorm_2,\n",
        "            'layernorm_2': self.layernorm_3, \n",
        "            'supports_masking': self.supports_masking,\n",
        "        }) \n",
        "        return config"
      ],
      "metadata": {
        "id": "A1GZaN-2-8Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "metadata": {
        "id": "zkF6xZdG--pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10  # This should be at least 30 for convergence\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFIoTvrv_EY_",
        "outputId": "df330b72-c2dc-4cbb-f5c3-815e33b0ca67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " positional_embedding (Position  (None, None, 128)   1922560     ['encoder_inputs[0][0]']         \n",
            " alEmbedding)                                                                                     \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Transform  (None, None, 128)   1054464     ['positional_embedding[0][0]']   \n",
            " erEncoder)                                                                                       \n",
            "                                                                                                  \n",
            " model_1 (Functional)           (None, None, 15000)  5439768     ['decoder_inputs[0][0]',         \n",
            "                                                                  'transformer_encoder[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,416,792\n",
            "Trainable params: 8,416,792\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "1690/1690 [==============================] - 144s 82ms/step - loss: 1.1703 - accuracy: 0.5176 - val_loss: 0.7813 - val_accuracy: 0.6155\n",
            "Epoch 2/10\n",
            "1690/1690 [==============================] - 138s 82ms/step - loss: 0.7553 - accuracy: 0.6285 - val_loss: 0.5838 - val_accuracy: 0.6789\n",
            "Epoch 3/10\n",
            "1690/1690 [==============================] - 138s 82ms/step - loss: 0.5904 - accuracy: 0.6774 - val_loss: 0.5051 - val_accuracy: 0.7064\n",
            "Epoch 4/10\n",
            "1690/1690 [==============================] - 138s 82ms/step - loss: 0.4950 - accuracy: 0.7082 - val_loss: 0.4724 - val_accuracy: 0.7209\n",
            "Epoch 5/10\n",
            "1690/1690 [==============================] - 138s 82ms/step - loss: 0.4334 - accuracy: 0.7304 - val_loss: 0.4440 - val_accuracy: 0.7324\n",
            "Epoch 6/10\n",
            "1690/1690 [==============================] - 138s 82ms/step - loss: 0.3896 - accuracy: 0.7471 - val_loss: 0.4390 - val_accuracy: 0.7367\n",
            "Epoch 7/10\n",
            "1690/1690 [==============================] - 138s 81ms/step - loss: 0.3562 - accuracy: 0.7607 - val_loss: 0.4313 - val_accuracy: 0.7419\n",
            "Epoch 8/10\n",
            "1690/1690 [==============================] - 138s 82ms/step - loss: 0.3305 - accuracy: 0.7722 - val_loss: 0.4275 - val_accuracy: 0.7454\n",
            "Epoch 9/10\n",
            "1690/1690 [==============================] - 137s 81ms/step - loss: 0.3091 - accuracy: 0.7823 - val_loss: 0.4273 - val_accuracy: 0.7467\n",
            "Epoch 10/10\n",
            "1690/1690 [==============================] - 137s 81ms/step - loss: 0.2917 - accuracy: 0.7907 - val_loss: 0.4223 - val_accuracy: 0.7502\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2cdf3c7d90>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(30):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(input_sentence, translated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp5tbn4xKpWV",
        "outputId": "9c9249f8-0e4b-49f3-ffc3-c221e0fea0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It isn't cold today. [start] Сьогодні не холодно [end]\n",
            "It's a serious question. [start] Це серйозна питання [end]\n",
            "Tom loves science fiction. [start] Том обожнює наукову фантастику [end]\n",
            "We realized that it was pointless. [start] Ми усвідомив що це не має сенсу [end]\n",
            "Do you know anything about Tom's childhood? [start] Ти щось знаєш про дитинство Тома [end]\n",
            "Tom isn't going to be happy. [start] Том не буде щасливий [end]\n",
            "Who was this machine invented by? [start] Хто був власником будинку метою [end]\n",
            "He tried hard, but failed. [start] Він здивувався але [UNK] але не було [end]\n",
            "Are you over 18? [start] Тобі більше 18 [end]\n",
            "Tom rejected this proposal. [start] Том поставив свою пропозицію [end]\n",
            "Did Tom stay? [start] Том лишився [end]\n",
            "I wake up every morning with a headache. [start] Я прокидаюся щоранку вранці світло [end]\n",
            "Tom was kicked out of the meeting. [start] Тома вигнали зі зустрічі [end]\n",
            "Which is better? [start] Що це за краще [end]\n",
            "It kept on raining for a week. [start] Дощ ішов цілий тиждень [end]\n",
            "Tom is our strongest defensive player. [start] Том — [UNK] [UNK] [UNK] [end]\n",
            "I talked with my parents about my studying abroad. [start] Я поговорив з батьками про твою навчання за кордон [end]\n",
            "What happened then? [start] Що сталося потім [end]\n",
            "I'm doubtful whether Tom will come. [start] Я сумніваюся що Том прийде [end]\n",
            "She gave him a sweater. [start] Вона дала йому светер [end]\n",
            "He lost his eyesight. [start] Він втратив зір [end]\n",
            "Your tie looks good. [start] У тебе добра краватка [end]\n",
            "Does Tom need me? [start] Я потрібна Тому [end]\n",
            "Can you speak French? [start] Ти говориш французькою [end]\n",
            "I think you know that that isn't true. [start] Я думаю ти знаєш що це неправда [end]\n",
            "It's as cold as ice. [start] Він такий холодний яблука [end]\n",
            "He just arrived. [start] Він щойно приїхав [end]\n",
            "Where was Tom born? [start] Де Том народився [end]\n",
            "Tom has a beautiful garden. [start] У Тома гарний сад [end]\n",
            "That's a pagoda. [start] Це [UNK] [end]\n"
          ]
        }
      ]
    }
  ]
}