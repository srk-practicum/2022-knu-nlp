{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "floating-bachelor",
   "metadata": {
    "id": "focused-verse"
   },
   "source": [
    "# Step 5 - Ukrainian-to-English translation with Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-continent",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cop-Rzl7zBCg",
    "outputId": "cd7e4caa-6803-4565-a26a-5dcca3344df5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-brighton",
   "metadata": {
    "id": "alike-invention"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-situation",
   "metadata": {
    "id": "revised-newman"
   },
   "outputs": [],
   "source": [
    "def show_percentage(m, n):\n",
    "    clear_output(wait=True)\n",
    "    k = m / (n - 1)\n",
    "    percent = int(k * 100)\n",
    "    print('[' + 'x'*percent + '-'*(100 - percent) + ']' + \" {:.2%}\".format(k))\n",
    "    print(\"{}/{} complete\".format(m + 1, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-wholesale",
   "metadata": {
    "id": "metric-bennett"
   },
   "source": [
    "### Parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-distribution",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "timely-prevention",
    "outputId": "54a2f378-3ea2-48ef-ec1d-bba906be9684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do it.\tДій.\tCC-BY 2.0 (France) Attribution: tatoeba.org #6049386 (mailohilohi) & #7555655 (miketheknight)\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/gdrive/MyDrive/practice_nlp_2022/data/ukr-eng/ukr.txt\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "    \n",
    "line1 = lines[25]\n",
    "print(line1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-lying",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1WV7lqJ2Z-B",
    "outputId": "7dc29fec-7f64-4d8e-e2ce-0db5099de81d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx] 100.00%\n",
      "154461/154461 complete\n"
     ]
    }
   ],
   "source": [
    "text_pairs = []\n",
    "n = len(lines)\n",
    "for i in range(n):\n",
    "    line = lines[i]\n",
    "    eng, ukr = line.split(\"\\t\")[:2]\n",
    "    ukr = \"[start] \" + ukr + \" [end]\"\n",
    "    text_pairs.append((eng, ukr))\n",
    "    show_percentage(i, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-interest",
   "metadata": {
    "id": "jwWdIHz08f72"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/gdrive/MyDrive/practice_nlp_2022/data/preprocessed_ukr.txt\", \"w\") as new_f:\n",
    "    for pair in text_pairs:\n",
    "        eng, ukr = pair\n",
    "        new_f.write(eng + \" \" + ukr + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-feedback",
   "metadata": {
    "id": "GUIKJ1IAOY6A"
   },
   "source": [
    "##### Split the sentence pairs into a training set, a validation set, and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-letters",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "musical-yacht",
    "outputId": "3ee31029-9f0e-4c06-8dd7-2431fffe9f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154461 total pairs\n",
      "108123 training pairs\n",
      "23169 validation pairs\n",
      "23169 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-uruguay",
   "metadata": {
    "id": "encouraging-assurance"
   },
   "source": [
    "### Vectorizing the text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-pleasure",
   "metadata": {
    "id": "vPBYk3x2OrIp"
   },
   "source": [
    "##### We'll use two instances of the TextVectorization layer to vectorize the text data (one for English and one for Ukrainian), that is to say, to turn the original strings into integer sequences where each integer represents the index of a word in a vocabulary.\n",
    "##### The English layer will use the default string standardization (strip punctuation characters) and splitting scheme (split on whitespace), while the Ukrainian layer will use a custom standardization, where we add the character \"¿\" to the set of punctuation characters to be stripped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-collective",
   "metadata": {
    "id": "knowing-harmony"
   },
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, \n",
    "    output_mode=\"int\", \n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "ukr_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_ukr_texts = [pair[1] for pair in train_pairs]\n",
    "eng_vectorization.adapt(train_eng_texts)\n",
    "ukr_vectorization.adapt(train_ukr_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-shooting",
   "metadata": {
    "id": "p-7NZB0RPFRb"
   },
   "source": [
    "##### Next, we'll format our datasets. At each training step, the model will seek to predict target words N+1 (and beyond) using the source sentence and the target words 0 to N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-pennsylvania",
   "metadata": {
    "id": "proud-metropolitan"
   },
   "outputs": [],
   "source": [
    "# As such, the training dataset will yield a tuple (inputs, targets), where:\n",
    "#    - inputs is a dictionary with the keys encoder_inputs and decoder_inputs. \n",
    "#      encoder_inputs is the vectorized source sentence and encoder_inputs is the \n",
    "#      target sentence \"so far\", that is to say, the words 0 to N used to predict \n",
    "#      word N+1 (and beyond) in the target sentence.\n",
    "#    - target is the target sentence offset by one step: it provides the next \n",
    "#      words in the target sentence - what the model will try to predict.\n",
    "\n",
    "\n",
    "def format_dataset(eng, ukr):\n",
    "    eng = eng_vectorization(eng)\n",
    "    ukr = ukr_vectorization(ukr)\n",
    "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ukr[:, :-1],}, ukr[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, ukr_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    ukr_texts = list(ukr_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ukr_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-margin",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "joined-promise",
    "outputId": "edba73b4-b02b-42b3-9692-94cbba4946dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "# Let's take a quick look at the sequence shapes \n",
    "# (we have batches of 64 pairs, and all sequences are 20 steps long)\n",
    "\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-glory",
   "metadata": {
    "id": "operating-sampling"
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-reserve",
   "metadata": {
    "id": "solved-graduate"
   },
   "outputs": [],
   "source": [
    "# Our sequence-to-sequence Transformer consists of a TransformerEncoder and a TransformerDecoder \n",
    "# chained together. To make the model aware of word order, we also use a PositionalEmbedding layer.\n",
    "\n",
    "# The source sequence will be pass to the TransformerEncoder, which will produce a new \n",
    "# representation of it. This new representation will then be passed to the TransformerDecoder, \n",
    "# together with the target sequence so far (target words 0 to N). The TransformerDecoder will then \n",
    "# seek to predict the next words in the target sequence (N+1 and beyond).\n",
    "\n",
    "# A key detail that makes this possible is causal masking (see method get_causal_attention_mask() \n",
    "# on the TransformerDecoder). The TransformerDecoder sees the entire sequences at once, and thus \n",
    "# we must make sure that it only uses information from target tokens 0 to N when predicting token N+1 \n",
    "# (otherwise, it could use information from the future, which would result in a model that cannot be used at inference time).\n",
    "\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"num_heads\": self.num_heads\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"num_heads\": self.num_heads\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-community",
   "metadata": {
    "id": "juvenile-department"
   },
   "outputs": [],
   "source": [
    "# Next, we assemble the end-to-end model.\n",
    "\n",
    "embed_dim = 128\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(\n",
    "    shape=(None,), \n",
    "    dtype=\"int64\", \n",
    "    name=\"encoder_inputs\"\n",
    ")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(\n",
    "    shape=(None,), \n",
    "    dtype=\"int64\", \n",
    "    name=\"decoder_inputs\"\n",
    ")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], \n",
    "    decoder_outputs, \n",
    "    name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-hamburg",
   "metadata": {
    "id": "accomplished-invitation"
   },
   "source": [
    "### Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-elevation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dimensional-signature",
    "outputId": "b4146504-de9f-4724-9fef-9bea1da0bfdc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "1690/1690 [==============================] - 143s 83ms/step - loss: 1.1636 - accuracy: 0.5208 - val_loss: 0.7869 - val_accuracy: 0.6142\n",
      "Epoch 2/17\n",
      "1690/1690 [==============================] - 135s 80ms/step - loss: 0.7491 - accuracy: 0.6304 - val_loss: 0.5942 - val_accuracy: 0.6754\n",
      "Epoch 3/17\n",
      "1690/1690 [==============================] - 135s 80ms/step - loss: 0.5857 - accuracy: 0.6788 - val_loss: 0.5139 - val_accuracy: 0.7012\n",
      "Epoch 4/17\n",
      "1690/1690 [==============================] - 135s 80ms/step - loss: 0.4920 - accuracy: 0.7097 - val_loss: 0.4760 - val_accuracy: 0.7162\n",
      "Epoch 5/17\n",
      "1690/1690 [==============================] - 135s 80ms/step - loss: 0.4313 - accuracy: 0.7315 - val_loss: 0.4544 - val_accuracy: 0.7268\n",
      "Epoch 6/17\n",
      "1690/1690 [==============================] - 135s 80ms/step - loss: 0.3880 - accuracy: 0.7475 - val_loss: 0.4456 - val_accuracy: 0.7325\n",
      "Epoch 7/17\n",
      "1690/1690 [==============================] - 135s 80ms/step - loss: 0.3555 - accuracy: 0.7617 - val_loss: 0.4374 - val_accuracy: 0.7386\n",
      "Epoch 8/17\n",
      "1690/1690 [==============================] - 136s 80ms/step - loss: 0.3297 - accuracy: 0.7730 - val_loss: 0.4337 - val_accuracy: 0.7417\n",
      "Epoch 9/17\n",
      "1690/1690 [==============================] - 135s 80ms/step - loss: 0.3085 - accuracy: 0.7824 - val_loss: 0.4347 - val_accuracy: 0.7426\n",
      "Epoch 10/17\n",
      "1690/1690 [==============================] - 135s 80ms/step - loss: 0.2906 - accuracy: 0.7919 - val_loss: 0.4363 - val_accuracy: 0.7445\n",
      "Epoch 11/17\n",
      "1690/1690 [==============================] - 135s 80ms/step - loss: 0.2759 - accuracy: 0.7988 - val_loss: 0.4420 - val_accuracy: 0.7465\n",
      "Epoch 12/17\n",
      "1690/1690 [==============================] - 135s 80ms/step - loss: 0.2638 - accuracy: 0.8047 - val_loss: 0.4460 - val_accuracy: 0.7491\n",
      "Epoch 13/17\n",
      "1690/1690 [==============================] - 135s 80ms/step - loss: 0.2512 - accuracy: 0.8110 - val_loss: 0.4453 - val_accuracy: 0.7503\n",
      "Epoch 14/17\n",
      "1690/1690 [==============================] - 135s 80ms/step - loss: 0.2408 - accuracy: 0.8171 - val_loss: 0.4482 - val_accuracy: 0.7512\n",
      "Epoch 15/17\n",
      "1690/1690 [==============================] - 135s 80ms/step - loss: 0.2317 - accuracy: 0.8218 - val_loss: 0.4490 - val_accuracy: 0.7522\n",
      "Epoch 16/17\n",
      "1690/1690 [==============================] - 135s 80ms/step - loss: 0.2238 - accuracy: 0.8261 - val_loss: 0.4469 - val_accuracy: 0.7558\n",
      "Epoch 17/17\n",
      "1690/1690 [==============================] - 135s 80ms/step - loss: 0.2154 - accuracy: 0.8309 - val_loss: 0.4506 - val_accuracy: 0.7519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3a35235cd0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 17\n",
    "\n",
    "transformer.compile(\n",
    "    optimizer = \"adam\", \n",
    "    loss = \"sparse_categorical_crossentropy\", \n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-hello",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dj7PvAAFGkv9",
    "outputId": "4996dfd9-f96b-4e37-84ab-bbc98a944a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_4 (Positi  (None, None, 128)   1922560     ['encoder_inputs[0][0]']         \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder_2 (Transfo  (None, None, 128)   1054464     ['positional_embedding_4[0][0]'] \n",
      " rmerEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " model_5 (Functional)           (None, None, 15000)  5439768     ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder_2[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,416,792\n",
      "Trainable params: 8,416,792\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-uganda",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "xFmv7j4IGlzJ",
    "outputId": "35e7b3bd-525e-44fd-e5c0-6c92ea7234c4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = transformer.history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-electron",
   "metadata": {
    "id": "hWOIVL275Cjb"
   },
   "source": [
    "## Decoding test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-belgium",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aware-going",
    "outputId": "2b9797b3-36eb-4f3e-a203-450aacfaa258",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to go on a trip with you. - [start] Я хочу піти в поїздку з тобою [end]\n",
      "He got a bad reputation. - [start] Він має поганий найкраща подруга [end]\n",
      "I don't think Tom can help us now. - [start] Не думаю що Том може нам зараз допомогти [end]\n",
      "What's your beef? - [start] Яка у тебе яловичина [end]\n",
      "Open the door, please. - [start] Двері відчиняються будь ласка [end]\n",
      "I'll show you a few photos. - [start] Я покажу вам кілька фотографій [end]\n",
      "I have a friend who works for NASA. - [start] У мене є подруга на рік [UNK] [end]\n",
      "Tom felt a sharp pain in his stomach. - [start] Том почувався [UNK] під час живіт [end]\n",
      "We all looked out the window. - [start] Ми всі [UNK] до вікна [end]\n",
      "Tom said it was just a prank. - [start] Том сказав що це був прикол [end]\n",
      "May I borrow your car? - [start] Можна я скористаюся вашою машиною [end]\n",
      "I will give you a bicycle for your birthday. - [start] Я подарую вам велосипед на день народження [end]\n",
      "Was there enough money? - [start] Грошей було достатньо [end]\n",
      "A cup of tea, please. - [start] [UNK] кави будь [UNK] будь ласка [end]\n",
      "Now I understand what you're trying to tell me. - [start] Тепер я розумію що ти намагаєшся мені казати [end]\n",
      "Tom studies at Harvard. - [start] Том вчиться в Гарварді [end]\n",
      "Why were you so sad yesterday? - [start] Чому ти вчора була така сумна [end]\n",
      "What are you going to do today? - [start] Що ти сьогодні робитимеш [end]\n",
      "Tom says that he's never been arrested. - [start] Том каже що ніколи не був засмучений [end]\n",
      "Tom died in 2009. - [start] Том помер у [UNK] році [end]\n",
      "He took away what little money I had. - [start] Він взяв трохи грошей яку я мав [end]\n",
      "Tom is a magician. - [start] Том — чарівник [end]\n",
      "What do you want to sell? - [start] Що ти хочеш продати [end]\n",
      "I didn't have to tell Tom, but I did. - [start] Я не мав казати Тому але я теж [end]\n",
      "I think I know that girl over there. - [start] Я думаю що я знаю це он там [end]\n",
      "There were twenty or so people there. - [start] Тут було так близько людей [end]\n",
      "Everybody's safe. - [start] Усі в безпеці [end]\n",
      "I'm tired of swimming. - [start] Я втомилася плавати [end]\n",
      "You like arguing with me, don't you? - [start] Тобі подобається сперечатися з тобою [end]\n",
      "Tom is losing control. - [start] Том втрачає [UNK] [end]\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's demonstrate how to translate. We simply \n",
    "# feed into the model the vectorized English sentence as well as the target token \"[start]\", \n",
    "# then we repeatedly generated the next token, until we hit the token \"[end]\".\n",
    "\n",
    "ukr_vocab = ukr_vectorization.get_vocabulary()\n",
    "ukr_index_lookup = dict(zip(range(len(ukr_vocab)), ukr_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = ukr_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = ukr_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(30):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    print(input_sentence + \" - \" + translated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-savannah",
   "metadata": {
    "id": "Ii0GrrzM5SKg"
   },
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-variety",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBQrgFmRTKqT",
    "outputId": "aa9e7874-5414-4bc8-8523-11636b4392ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, multi_head_attention_6_layer_call_fn while saving (showing 5 of 150). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: content/gdrive/MyDrive/practice_nlp_2022/data/ukr_eng_dir/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: content/gdrive/MyDrive/practice_nlp_2022/data/ukr_eng_dir/assets\n"
     ]
    }
   ],
   "source": [
    "keras_model_path = \"content/gdrive/MyDrive/practice_nlp_2022/data/ukr_eng_dir\"\n",
    "transformer.save(keras_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-indianapolis",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 834
    },
    "id": "8WJ_y-5XURve",
    "outputId": "f2d62ed3-b429-4187-d553-5b4bbe594c95"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-b0ac4d8f6a8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrestored_keras_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/function_deserialization.py\u001b[0m in \u001b[0;36mrestored_function_body\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m           .format(index + 1, _pretty_format_positional(positional), keyword))\n\u001b[1;32m    289\u001b[0m     raise ValueError(\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;34m\"Could not find matching concrete function to call loaded from the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0;34mf\"SavedModel. Got:\\n  {_pretty_format_positional(args)}\\n  Keyword \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;34mf\"arguments: {kwargs}\\n\\n Expected these arguments to match one of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"transformer_decoder_2\" (type TransformerDecoder).\n\nCould not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (4 total):\n    * Tensor(\"inputs:0\", shape=(None, None, 128), dtype=float32)\n    * Tensor(\"encoder_outputs:0\", shape=(None, None, 128), dtype=float32)\n    * None\n    * False\n  Keyword arguments: {}\n\n Expected these arguments to match one of the following 2 option(s):\n\nOption 1:\n  Positional arguments (4 total):\n    * TensorSpec(shape=(None, None, 128), dtype=tf.float32, name='inputs')\n    * TensorSpec(shape=(None, None, 128), dtype=tf.float32, name='encoder_outputs')\n    * TensorSpec(shape=(None, None), dtype=tf.bool, name='mask')\n    * False\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (4 total):\n    * TensorSpec(shape=(None, None, 128), dtype=tf.float32, name='inputs')\n    * TensorSpec(shape=(None, None, 128), dtype=tf.float32, name='encoder_outputs')\n    * TensorSpec(shape=(None, None), dtype=tf.bool, name='mask')\n    * True\n  Keyword arguments: {}\n\nCall arguments received:\n  • args=('tf.Tensor(shape=(None, None, 128), dtype=float32)',)\n  • kwargs={'encoder_outputs': 'tf.Tensor(shape=(None, None, 128), dtype=float32)', 'training': 'None'}"
     ]
    }
   ],
   "source": [
    "restored_keras_model = tf.keras.models.load_model(keras_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-latin",
   "metadata": {
    "id": "EwqylPipURye"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "step_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
