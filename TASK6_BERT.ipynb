{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TASK6-BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing data from TASK1\n"
      ],
      "metadata": {
        "id": "gEELrCMmke_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk import WordNetLemmatizer\n",
        "import re\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "  \n",
        "url='https://raw.githubusercontent.com/srk-practicum/2022-knu-nlp/kachaikin_branch/Task1_Kachaikin/root2ai%20-%20Data.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "U=list(df['Target'])\n",
        "A = list(set(df['Target']))\n",
        "\n",
        "#Let's chose 2 very different labels, for example Bigdata and Cyber Security\n",
        "\n",
        "V = list(df['Text'])\n",
        "\n",
        "R = [[V[i],U[i]] for i in range(len(U))]\n",
        "new_frame = [i for i in R if i[1]== 'Bigdata' or i[1]== 'Cyber Security']\n",
        "new_frame_texts = [i[0] for i in new_frame]\n",
        "new_frame_labels = [i[1] for i in new_frame]\n",
        "new_frame_labels_asreal = []\n",
        "for i in new_frame_labels:\n",
        "    if i == 'Bigdata':\n",
        "      new_frame_labels_asreal.append(1.0)\n",
        "    elif i == 'Cyber Security':\n",
        "      new_frame_labels_asreal.append(0.0)\n",
        "# Okey, now we have new dataframe with labels Bigdata = 1.0 and Cyber Security = 0.0\n",
        "df = pd.DataFrame(data = {'Text':new_frame_texts , 'Target':new_frame_labels_asreal})\n",
        "\n",
        "#Let's watch how distributed labels\n",
        "number_of_bd = 0\n",
        "number_of_cs = 0\n",
        "for i in list(df['Target']):\n",
        "  if float(i) == 1.0:\n",
        "    number_of_bd+=1\n",
        "  elif float(i) == 0.0:\n",
        "    number_of_cs +=1\n",
        "\n",
        "text1 = ''\n",
        "for i in new_frame_texts:\n",
        "  text1+=i\n",
        "\n",
        "U = list(df['Text'])\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "V = list(df['Target'])\n",
        "\n",
        "U_f = []\n",
        "for j in U:\n",
        "  R = nltk.word_tokenize(j)\n",
        "  Y = [lemmatizer.lemmatize(i) for i in R]\n",
        "  StrY = \" \".join(Y)\n",
        "  U_f.append(StrY)\n",
        "\n",
        "stop_words = stopwords.words(\"english\")\n",
        "\n",
        "def cleaning(data):\n",
        "    \n",
        "    filtered = re.sub(\"@\\S+\", \" \", data)  # remove mentions\n",
        "    filtered = re.sub(\"https*\\S+\", \" \", filtered) # remove url\n",
        "    filtered = re.sub(\"#\\S+\", \" \", filtered) # remove hashtags\n",
        "    filtered = re.sub(\"\\d\", \" \", filtered) # remove all numbers\n",
        "    filtered = re.sub('[%s]+' % re.escape(string.punctuation), ' ', filtered)  # remove punctuation\n",
        "    filtered = re.sub('\\n', ' ', filtered) # remove new lines       \n",
        "    filtered = re.sub('\\s{2,}',' ', filtered) # remove extra spaces\n",
        "    \n",
        "    filtered = filtered.lower()\n",
        "    \n",
        "    filtered = ' '.join([word for word in filtered.split(' ') if word not in stop_words])\n",
        "    \n",
        "    return filtered\n",
        "\n",
        "df = pd.DataFrame(data = {'Text':[cleaning(i) for i in U_f], 'Target':V})\n",
        "\n",
        "text_dataset = np.array(df['Text'])\n",
        "label_dataset = np.array(df['Target'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym8ta7Y9kmHv",
        "outputId": "3484f765-899a-4bbb-83ca-1c84cc802b86"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "text_dataset_train, text_dataset_test, label_dataset_train, label_dataset_test = train_test_split(df['Text'], \n",
        "                                                    df['Target'], test_size=0.1 ,\n",
        "                                                    random_state=42)\n",
        "text_dataset_train, text_dataset_test, label_dataset_train, label_dataset_test = np.array(text_dataset_train), np.array(text_dataset_test), np.asarray(label_dataset_train).astype('int32'), np.asarray(label_dataset_test).astype('int32')\n"
      ],
      "metadata": {
        "id": "mBD4iOpBoT0T"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "ORoUCNHMkm8u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a2CwREmkEMy",
        "outputId": "07a9be56-3eff-4b9a-a0be-662f830dde00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4416, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# !pip3 install transformers - that command we use for installing transformers to colab\n",
        "import random\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "def tokenize(sequence):\n",
        "    tokens = tokenizer.encode_plus(sequence, max_length=128,\n",
        "                                   truncation=True, padding='max_length',\n",
        "                                   add_special_tokens=True, return_token_type_ids=False,\n",
        "                                   return_tensors='tf')\n",
        "    return tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "num_of_elements = len(text_dataset_train)\n",
        "\n",
        "Xids_train = np.zeros((num_of_elements, 128))\n",
        "Xmask_train = np.zeros((num_of_elements, 128))\n",
        "\n",
        "for i, sequence in enumerate(text_dataset_train):\n",
        "    tokens = tokenize(sequence)\n",
        "    Xids_train[i, :], Xmask_train[i, :] = tokens[0], tokens[1]\n",
        "\n",
        "Xids_train.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "bert = TFAutoModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "input_ids = tf.keras.layers.Input(shape=(128,), name='input_ids', dtype='int32')\n",
        "mask = tf.keras.layers.Input(shape=(128,), name='attention_mask', dtype='int32')\n",
        "\n",
        "embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # we access the transformer model within our bert object using the bert attribute (eg bert.bert instead of bert)\n",
        "\n",
        "# Classifier head\n",
        "x = tf.keras.layers.Dense(16, activation ='relu')(embeddings)\n",
        "y = tf.keras.layers.Dense(1, activation ='sigmoid', name='outputs')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "\n",
        "model.layers[2].trainable = False\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "history = model.fit(\n",
        "    [Xids_train, Xmask_train], label_dataset_train,\n",
        "    validation_split=0.2,\n",
        "    batch_size = 1024,\n",
        "    epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGQjJgVjqh39",
        "outputId": "0122dc7d-ada5-4416-d602-ca5535b964b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 128,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 16)           12304       ['bert[0][1]']                   \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 1)            17          ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,322,593\n",
            "Trainable params: 12,321\n",
            "Non-trainable params: 108,310,272\n",
            "__________________________________________________________________________________________________\n",
            "4/4 [==============================] - 2110s 532s/step - loss: 0.7124 - accuracy: 0.5396 - val_loss: 0.7330 - val_accuracy: 0.4468\n"
          ]
        }
      ]
    }
  ]
}