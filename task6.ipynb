{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6999d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "import re, string\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27bd2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the training urls\n",
    "\n",
    "train_url = 'https://drive.google.com/file/d/1F1fDHnvDMEopHYaulMQYNy1SCpEenoFr/view?usp=sharing'\n",
    "train_url_ = 'https://drive.google.com/uc?id=' + train_url.split('/')[-2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc6ac172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the testing urls\n",
    "test_url = 'https://drive.google.com/file/d/1Jg3l_AmdkfEIIiLAuwlwyybpXYt93Vvp/view?usp=sharing'\n",
    "test_url_ = 'https://drive.google.com/uc?id=' + test_url.split('/')[-2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "067bb3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my training link: https://drive.google.com/uc?id=1F1fDHnvDMEopHYaulMQYNy1SCpEenoFr\n",
      "my testing link: https://drive.google.com/uc?id=1Jg3l_AmdkfEIIiLAuwlwyybpXYt93Vvp\n"
     ]
    }
   ],
   "source": [
    "print('my training link:', train_url_)\n",
    "print('my testing link:', test_url_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e89961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0                                                  1\n",
      "0  __label__0  study interventions are recombinant CD40-ligan...\n",
      "1  __label__0  study interventions are Liposomal doxorubicin ...\n",
      "2  __label__0  study interventions are BI 836909 . multiple m...\n",
      "3  __label__0  study interventions are Immunoglobulins . recu...\n",
      "4  __label__0  study interventions are Paclitaxel . stage ova...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# reading the files\n",
    "\n",
    "df_train = pd.read_csv(train_url_, header= None) # got the dataframe\n",
    "df_test = pd.read_csv(test_url_, header= None)\n",
    "print(df_train.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31772602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> \n",
      " 0    study interventions are recombinant CD40-ligan...\n",
      "1    study interventions are Liposomal doxorubicin ...\n",
      "2    study interventions are BI 836909 . multiple m...\n",
      "3    study interventions are Immunoglobulins . recu...\n",
      "4    study interventions are Paclitaxel . stage ova...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# now we are workking with the second column (main data)\n",
    "\n",
    "train_data_set = df_train[1]\n",
    "test_data_set = df_test[1]\n",
    "\n",
    "print(type(train_data_set),'\\n', train_data_set[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1e2dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type,length of training labels: <class 'numpy.ndarray'> 9907 [0. 0. 0. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# working with labels\n",
    "\n",
    "train_label = []\n",
    "test_label = []\n",
    "\n",
    "# in order to the program to distinguish labels\n",
    "\n",
    "for label in list(df_train[0]):\n",
    "    if label == '__label__0':\n",
    "        train_label.append(0)\n",
    "    if label == '__label__1':\n",
    "        train_label.append(1)\n",
    "\n",
    "for label_test in list(df_test[0]):\n",
    "    if label_test == '__label__0':\n",
    "        test_label.append(0)\n",
    "    if label_test == '__label__1':\n",
    "        test_label.append(1)\n",
    "\n",
    "# print(type(train_label), len(train_label))\n",
    "# print(type(test_label))\n",
    "\n",
    "y_train = np.asarray(train_label).astype(\"float32\")\n",
    "y_test = np.asarray(test_label).astype(\"float32\")\n",
    "\n",
    "print('type,length of training labels:', type(y_train),len(y_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48a1eccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> \n",
      " 0    study interventions recombinant cd ligand mela...\n",
      "1    study interventions liposomal doxorubicin colo...\n",
      "2    study interventions bi multiple myeloma diagno...\n",
      "3    study interventions immunoglobulins recurrent ...\n",
      "4    study interventions paclitaxel stage ovarian c...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# preprocessing text \n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "def cleaning(data):\n",
    "    \n",
    "    filtered = re.sub(\"@\\S+\", \" \", data)  # remove mentions\n",
    "    filtered = re.sub(\"https*\\S+\", \" \", filtered) # remove url\n",
    "    filtered = re.sub(\"#\\S+\", \" \", filtered) # remove hashtags\n",
    "    filtered = re.sub(\"\\d\", \" \", filtered) # remove all numbers\n",
    "    filtered = re.sub('[%s]' % re.escape(string.punctuation), ' ', filtered)  # remove punctuation\n",
    "    filtered = re.sub('\\n', ' ', filtered) # remove new lines       \n",
    "    filtered = re.sub('\\s{2,}',' ', filtered) # remove extra spaces\n",
    "    \n",
    "    filtered = filtered.lower()\n",
    "    \n",
    "    filtered = ' '.join([word for word in filtered.split(' ') if word not in stop_words])\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "cleaned_train = train_data_set.apply(cleaning)\n",
    "cleaned_test = test_data_set.apply(cleaning)\n",
    " \n",
    "print(type(cleaned_train), '\\n', cleaned_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c09c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_in_string = ''\n",
    "for i in cleaned_train:\n",
    "    train_in_string += (' ' + i)\n",
    "    \n",
    "test_in_string = ''\n",
    "for j in cleaned_test:\n",
    "    test_in_string += (' ' + j)\n",
    "    \n",
    "all_data = train_in_string + test_in_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0fc3d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_data type: <class 'pandas.core.series.Series'>\n",
      "x_test_data type: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenization = word_tokenize(all_data)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_all_data = [lemmatizer.lemmatize(t) for t in tokenization]\n",
    "dictionary = list(set(lemmatized_all_data))\n",
    "\n",
    "# func is showing the dataframe of word indexes\n",
    "def lemmatization(data):\n",
    "\n",
    "    data = data.lower()\n",
    "    tokens = word_tokenize(data)\n",
    "    lemmatized = [lemmatizer.lemmatize(l) for l in tokens]\n",
    "    res = [dictionary.index(i) for i in lemmatized]\n",
    "    return res\n",
    "    \n",
    "\n",
    "x_train_data = cleaned_train.apply(lemmatization)\n",
    "x_test_data = cleaned_test.apply(lemmatization)\n",
    "\n",
    "print('x_train_data type:', type(x_train_data))\n",
    "print('x_test_data type:', type(x_test_data))\n",
    "\n",
    "# print(type(x_test_data), len(x_test_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6b664eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9907, 128)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# !pip3 install transformers - that command we use for installing transformers to colab\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "def tokenize(sequence):\n",
    "    tokens = tokenizer.encode_plus(sequence, max_length=128,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_token_type_ids=False,\n",
    "                                   return_tensors='tf')\n",
    "    return tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "num_of_elements = len(cleaned_train)\n",
    "\n",
    "Xids_train = np.zeros((num_of_elements, 128))\n",
    "Xmask_train = np.zeros((num_of_elements, 128))\n",
    "\n",
    "for i, sequence in enumerate(cleaned_train):\n",
    "    tokens = tokenize(sequence)\n",
    "    Xids_train[i, :], Xmask_train[i, :] = tokens[0], tokens[1]\n",
    "\n",
    "Xids_train.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7172d4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 8)            6152        ['bert[0][1]']                   \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 1)            9           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,316,433\n",
      "Trainable params: 6,161\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n",
      "8/8 [==============================] - 973s 123s/step - loss: 0.7120 - accuracy: 0.5572 - val_loss: 1.1029 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import TFAutoModel\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "bert = TFAutoModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "input_ids = tf.keras.layers.Input(shape=(128,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(128,), name='attention_mask', dtype='int32')\n",
    "\n",
    "embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # we access the transformer model within our bert object using the bert attribute (eg bert.bert instead of bert)\n",
    "\n",
    "# Classifier head\n",
    "x = tf.keras.layers.Dense(8, activation ='relu')(embeddings)\n",
    "y = tf.keras.layers.Dense(1, activation ='sigmoid', name='outputs')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "history = model.fit(\n",
    "    [Xids_train, Xmask_train], y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size = 1024,\n",
    "    epochs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd9a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
